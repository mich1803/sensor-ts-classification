{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20f62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.linalg import lstsq\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd8064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_csv(file_list, savepath, min_rows=200, normalize_labels=True):\n",
    "    \"\"\"\n",
    "    Aggregates multiple CSV files into a single CSV file.\n",
    "    - Each CSV must have the same structure, with the last column being the label.\n",
    "    - Interpolates missing values in features.\n",
    "    - Ensures each experiment name is unique.\n",
    "    - Only includes files with at least `min_rows` rows.\n",
    "    - Normalizes labels by replacing spaces with underscores (optional).\n",
    "    \"\"\"\n",
    "    aggregated_rows = []\n",
    "    colnames = None\n",
    "    used_exp_names = set()\n",
    "\n",
    "    for file in tqdm(file_list, desc=\"Aggregating CSVs\"):\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=';')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if len(df) < min_rows:\n",
    "            continue\n",
    "\n",
    "        # Interpolate features (exclude last column)\n",
    "        df.iloc[:, :-1] = df.iloc[:, :-1].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        if df.iloc[:, :-1].isnull().values.any():\n",
    "            print(f\"Warning: {file} still contains NaNs after interpolation\")\n",
    "            continue\n",
    "\n",
    "        features = df.iloc[:, :-1].values\n",
    "        label_raw = df.iloc[0, -1]\n",
    "\n",
    "        # Normalize label\n",
    "        if normalize_labels:\n",
    "            label = str(label_raw).strip().replace(\" \", \"_\").upper()\n",
    "        else:\n",
    "            label = str(label_raw).strip()\n",
    "\n",
    "        # Unique experiment name\n",
    "        base_exp_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        exp_name = base_exp_name\n",
    "        while exp_name in used_exp_names:\n",
    "            exp_name += \"_\"\n",
    "        used_exp_names.add(exp_name)\n",
    "\n",
    "        for row in features:\n",
    "            aggregated_rows.append(list(row) + [label, exp_name])\n",
    "\n",
    "        if colnames is None:\n",
    "            feature_names = df.columns[:-1]\n",
    "            colnames = list(feature_names) + [\"label\", \"exp_name\"]\n",
    "\n",
    "    # Create and save aggregated DataFrame\n",
    "    df_agg = pd.DataFrame(aggregated_rows, columns=colnames)\n",
    "    df_agg.to_csv(savepath, index=False)\n",
    "    print(f\"✅ Aggregated CSV saved to {savepath} (only experiments with ≥ {min_rows} rows)\")\n",
    "\n",
    "def rbf_kernel(x, c, r=0.1, kind='gaussian'):\n",
    "    dist = np.abs(x - c)\n",
    "    if kind == 'gaussian':\n",
    "        return np.exp(- (dist ** 2) / (2 * r ** 2))\n",
    "    elif kind == 'multiquadric':\n",
    "        return np.sqrt(dist ** 2 + r ** 2)\n",
    "    elif kind == 'inverse_multiquadric':\n",
    "        return 1 / np.sqrt(dist ** 2 + r ** 2)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported RBF type\")\n",
    "\n",
    "def approximate_with_rbf(signal, num_centers=5, rbf_type='gaussian', r=100.0):\n",
    "    signal = np.asarray(signal).copy()\n",
    "    n = len(signal)\n",
    "    x = np.arange(n)\n",
    "\n",
    "    if np.any(np.isnan(signal)):\n",
    "        not_nan = ~np.isnan(signal)\n",
    "        if np.sum(not_nan) == 0:\n",
    "            raise ValueError(\"Signal contains only NaNs\")\n",
    "        f_interp = interp1d(x[not_nan], signal[not_nan], kind='linear', fill_value='extrapolate')\n",
    "        signal = f_interp(x)\n",
    "\n",
    "    centers = np.linspace(0, n - 1, num_centers)\n",
    "    A = np.zeros((n, num_centers))\n",
    "    for j, c in enumerate(centers):\n",
    "        A[:, j] = rbf_kernel(x, c, r=r, kind=rbf_type)\n",
    "\n",
    "    coeffs, _, _, _ = lstsq(A, signal)\n",
    "    return coeffs\n",
    "\n",
    "def extract_rbf_features(df, num_features=16, num_centers=5, rbf_type='gaussian', r=100.0):\n",
    "    feature_cols = df.columns[:num_features]\n",
    "    label_col = df.columns[num_features]\n",
    "    exp_name_col = df.columns[num_features + 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for exp_name, group in df.groupby(exp_name_col):\n",
    "        row = []\n",
    "\n",
    "        # Extract RBF coefficients for each feature column\n",
    "        for col in feature_cols:\n",
    "            signal = group[col].values\n",
    "            coeffs = approximate_with_rbf(signal, num_centers=num_centers, rbf_type=rbf_type, r=r)\n",
    "            row.extend(coeffs)\n",
    "\n",
    "        # Add label and exp_name (assumed constant in the group)\n",
    "        label = group[label_col].iloc[0]\n",
    "        row.append(label)\n",
    "        row.append(exp_name)\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    # Construct column names\n",
    "    rbf_feature_names = [\n",
    "        f\"{col}_rbf{i}\" for col in feature_cols for i in range(num_centers)\n",
    "    ]\n",
    "    final_cols = rbf_feature_names + [\"label\", \"exp_name\"]\n",
    "    \n",
    "    return pd.DataFrame(results, columns=final_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating CSVs: 100%|██████████| 738/738 [00:19<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Aggregated CSV saved to original_training_set.csv (only experiments with ≥ 200 rows)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Take the original dataset and aggregate all the CSV files into a single CSV file.\n",
    "This is done for both the training and test sets.\n",
    "\"\"\"\n",
    "\n",
    "SCA_dset_dir = \"../dataset/SCA\"\n",
    "\n",
    "train_csvs = []\n",
    "test_csvs = []\n",
    "\n",
    "for split in ['TRAINING', 'TEST']:\n",
    "    split_dir = os.path.join(SCA_dset_dir, split, \"NORMALIZED\")\n",
    "    \n",
    "    for label in os.listdir(split_dir):\n",
    "        label_dir = os.path.join(split_dir, label)\n",
    "        \n",
    "        \n",
    "        if os.path.isdir(label_dir):\n",
    "            for filename in os.listdir(label_dir):\n",
    "                if filename.endswith('.csv'):\n",
    "                    file_path = os.path.join(label_dir, filename)\n",
    "                    if split == 'TRAINING':\n",
    "                        train_csvs.append(file_path)\n",
    "                    else:\n",
    "                        test_csvs.append(file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "aggregate_csv(file_list=train_csvs, savepath=\"original_training_set.csv\")\n",
    "#aggregate_csv(file_list=test_csvs, savepath=os.path.join(\"original_test_set.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4a5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset\n",
    "df = pd.read_csv(\"original_training_set.csv\")\n",
    "\n",
    "# Get unique exp_names and their labels\n",
    "exp_labels = df.groupby(\"exp_name\")[\"label\"].first()\n",
    "\n",
    "# Prepare split dict\n",
    "split_dict = defaultdict(list)\n",
    "\n",
    "# Step 1: Filter out labels with fewer than 10 exp_names\n",
    "label_counts = exp_labels.value_counts()\n",
    "\n",
    "# Select valid labels (at least 10 exp_names)\n",
    "valid_labels = label_counts[label_counts >= 10].index\n",
    "\n",
    "# Keep only rows with valid labels\n",
    "df_filtered = df[df[\"exp_name\"].isin(exp_labels[exp_labels.isin(valid_labels)].index)]\n",
    "\n",
    "# Save the filtered dataset (optional)\n",
    "df_filtered.to_csv(\"original_training_set_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27c9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_rbf_features(df = pd.read_csv(\"original_training_set_filtered.csv\"), num_centers=5, rbf_type='gaussian', r=100.0).to_csv(\"rbf_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa66f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  339\n",
      "Val:  41\n",
      "Test:  41\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"rbf_features.csv\")\n",
    "\n",
    "# Load the split dictionary\n",
    "split_dict_path = \"../instant_approach/splits/split_dict.json\"\n",
    "with open(split_dict_path, 'r') as f:\n",
    "    split_dict = json.load(f)\n",
    "\n",
    "train_df = df[df[\"exp_name\"].isin(split_dict[\"train\"])]\n",
    "val_df = df[df[\"exp_name\"].isin(split_dict[\"val\"])]\n",
    "test_df = df[df[\"exp_name\"].isin(split_dict[\"test\"])]\n",
    "\n",
    "# Save the split datasets\n",
    "os.makedirs(\"splits\", exist_ok=True)\n",
    "json.dump(split_dict, open(\"splits/split_dict.json\", \"w\"), indent=4)\n",
    "train_df.to_csv(\"splits/train.csv\", index=False)\n",
    "val_df.to_csv(\"splits/val.csv\", index=False)\n",
    "test_df.to_csv(\"splits/test.csv\", index=False)\n",
    "\n",
    "print(\"Train: \", len(train_df))\n",
    "print(\"Val: \", len(val_df))\n",
    "print(\"Test: \", len(test_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
